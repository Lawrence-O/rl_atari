# Rainbow DQN configuration for Phoenix

# Environment settings
env_name: "ALE/Phoenix-v5"
frame_skip: 4
frame_stack: 4
episode_life: true
clip_rewards: true

# Core agent parameters
gamma: 0.99
learning_rate: 0.0001  # 6.25e-5, typically lower than vanilla DQN
batch_size: 32
target_update_freq: 8000

# Replay buffer parameters
buffer_size: 100000
alpha: 0.6              # PER: Priority exponent (how much to use priorities)
beta: 0.4               # PER: Initial importance sampling weight
beta_annealing: 1250000   # PER: Number of steps to anneal beta to 1.0 10000000
td_error_epsilon: 1e-6  # PER: Small constant to add to priorities

# N-step returns
n_steps: 3              # Number of steps for n-step returns (was 4)


# Training parameters
# num_episodes: 5000
# evaluation_freq: 100          # Evaluate every 200 episodes
# save_freq: 100                # Save every 200 episodes



max_steps_per_episode: 10000  # Max number of steps per episode
num_evaluation_episodes: 10   # Number of episodes to evaluate
log_freq: 10                  # Log every episode
video_record_frequency: 2  # Record every X evaluations

eval_frequency_frames: 1000000  # Evaluate every 1M frames (replaces episode-based eval)
save_frequency_frames: 2000000  # Save every 2M frames
max_frames: 50000000           # Total frames to train for (typically 50M in papers)

# Hardware and randomness
seed: 42
device: "cuda"  # Use "cpu" if no GPU availablen_steps: 4 
